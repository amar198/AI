{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stpwords(sent):\n",
    "    stp_wrds = set(stopwords.words('english'))    # retrieving unique stop words in english.\n",
    "    wrd_tkns = nltk.word_tokenize(sent)       # tokenize sentence passed to this fn.\n",
    "    \n",
    "    filtered_sentence = []                  # initializing an empty list for storing key words (excluding stop words).\n",
    "    \n",
    "    for wrd in wrd_tkns:\n",
    "        if wrd not in stp_wrds:             # if the word is a stop word it will not be added to the filtered list.\n",
    "            filtered_sentence.append(wrd)\n",
    "            \n",
    "    return filtered_sentence                # returning the list without stop-words of english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LESK algo returns the best definition of the sense in which the words are supposed to be used.\n",
    "# it returns a list which contains Tuples with word and its definition.\n",
    "# E.g.\n",
    "# [\n",
    "#     (word1, best definition of word1), - Tuple 1\n",
    "#     (word2, best definition of word2), - Tuple 2\n",
    "#     ...\n",
    "# ]\n",
    "\n",
    "# List of parameters passed to the LESK algo fn.\n",
    "# words: the list of words for which sense definition needs to be found.\n",
    "# sents: the sentence in which the association b/w word and sense will be found.\n",
    "\n",
    "def lesk_algo(words, sent):\n",
    "    wrd_def = []                           # var in which word and its definition will be stored.\n",
    "    split_sent = remove_stpwords(sent.lower())     # splitting the sentence and removing the stop words.\n",
    "        \n",
    "    for word in words:                     # finding synsets for each word passed in the 'words' variable.\n",
    "    # {\n",
    "        max_overlap = -1                   # initialized as -1, cause overlap may remain 0 after finding the \n",
    "                                           # intersection b/w sense definition and the word, for all the senses.\n",
    "                                           # In such a case no sense definition will be returned. Hence, initializing \n",
    "                                           # it as -1 so that the 1st sense is returned. \n",
    "                                           # (value of overlap = 0 and max_overlap = -1).\n",
    "        best_sense = ''                    # initializing variable.\n",
    "        \n",
    "        for sense in wordnet.synsets(word.lower()):\n",
    "        # {\n",
    "            overlap = len(                    # finding the length of intersected words and assigning it to a var.\n",
    "                set(                          \n",
    "                    nltk.word_tokenize(sense.definition()) # finding the length of unique words in the sense \n",
    "                                                           # definition text.\n",
    "                ).intersection(                            # finding the words which intersets with the words\n",
    "                    split_sent                             # retrieved from the sentences passed to this fn.\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if overlap > max_overlap:            # if another sense has more common words, then new sense is \n",
    "                max_overlap = overlap            # assigned to the best_sense variable to hold the sense till a \n",
    "                best_sense = sense.definition()  # better one is identified.\n",
    "        # } End of sense for-loop.\n",
    "        \n",
    "        wrd_def.append(best_sense)\n",
    "    # } End of word for-loop\n",
    "    \n",
    "    return list(zip(words, wrd_def))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LESK algo returns the best definition of the sense in which the words are supposed to be used.\n",
    "# it returns a list which contains Tuples with word and its definition.\n",
    "# E.g.\n",
    "# [\n",
    "#     (word1, best definition of word1), - Tuple 1\n",
    "#     (word2, best definition of word2), - Tuple 2\n",
    "#     ...\n",
    "# ]\n",
    "\n",
    "# List of parameters passed to the LESK algo fn.\n",
    "# sents: the sentence in which the association b/w word and sense will be found.\n",
    "\n",
    "def lesk_algo_sent(sent):\n",
    "    wrd_def = []                           # var in which word and its definition will be stored.\n",
    "    wrds4def = split_sent = remove_stpwords(sent.lower())     # splitting the sentence and removing the stop words.\n",
    "    \n",
    "    wrd_lst = []\n",
    "    prev_next_wrd = []\n",
    "    \n",
    "    for key, wrd in enumerate(split_sent):\n",
    "        if key == 0:\n",
    "            prev_next_wrd = [split_sent[key+1]]\n",
    "            \n",
    "        elif key > 0 and key < (len(split_sent)-1):\n",
    "            prev_next_wrd = [split_sent[key-1], split_sent[key+1]]\n",
    "            \n",
    "        elif key == (len(split_sent)-1):\n",
    "            prev_next_wrd = [split_sent[key-1]]\n",
    "            \n",
    "        wrd_lst.append(tuple([prev_next_wrd, wrd]))\n",
    "        \n",
    "    print(wrd_lst)\n",
    "    \n",
    "    for word in split_sent:                     # finding synsets for each word passed in the 'words' variable.\n",
    "    # {\n",
    "        max_overlap = -1                   # initialized as -1, cause overlap may remain 0 after finding the \n",
    "                                           # intersection b/w sense definition and the word, for all the senses.\n",
    "                                           # In such a case no sense definition will be returned. Hence, initializing \n",
    "                                           # it as -1 so that the 1st sense is returned. \n",
    "                                           # (value of overlap = 0 and max_overlap = -1).\n",
    "        best_sense = ''                    # initializing variable.\n",
    "        \n",
    "        for sense in wordnet.synsets(word.lower()):\n",
    "        # {\n",
    "            overlap = len(                    # finding the length of intersected words and assigning it to a var.\n",
    "                set(                          \n",
    "                    nltk.word_tokenize(sense.definition()) # finding the length of unique words in the sense \n",
    "                                                           # definition text.\n",
    "                ).intersection(                            # finding the words which intersets with the words\n",
    "                    wrds4def                               # retrieved from the sentences passed to this fn.\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if overlap > max_overlap:            # if another sense has more common words, then new sense is \n",
    "                max_overlap = overlap            # assigned to the best_sense variable to hold the sense till a \n",
    "                best_sense = sense.definition()  # better one is identified.\n",
    "                \n",
    "        # } End of sense for-loop.        \n",
    "        \n",
    "        wrds4def = wrds4def + remove_stpwords(best_sense)\n",
    "        wrd_def.append(best_sense)\n",
    "    # } End of word for-loop\n",
    "    \n",
    "    return list(zip(split_sent, wrd_def))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The frog is jumping around the bank of the river\"\n",
    "words = ['bank', 'jumping', 'leaping', 'frog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bank', 'enclose with a bank'),\n",
       " ('jumping', 'the act of jumping; propelling yourself off the ground'),\n",
       " ('leaping', 'a light, self-propelled movement upwards or forwards'),\n",
       " ('frog',\n",
       "  'any of various tailless stout-bodied amphibians with long hind limbs for leaping; semiaquatic and terrestrial species')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk_algo(words, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['jumping'], 'frog'), (['frog', 'around'], 'jumping'), (['jumping', 'bank'], 'around'), (['around', 'river'], 'bank'), (['bank'], 'river')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('frog',\n",
       "  'any of various tailless stout-bodied amphibians with long hind limbs for leaping; semiaquatic and terrestrial species'),\n",
       " ('jumping', 'the act of jumping; propelling yourself off the ground'),\n",
       " ('around', 'all around or on all sides'),\n",
       " ('bank', 'a long ridge or pile'),\n",
       " ('river', 'a large natural stream of water (larger than a creek)')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ['frog', 'jumping', 'around', 'bank', 'river']\n",
    "lesk_algo_sent(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
